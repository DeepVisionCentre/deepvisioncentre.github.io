---
layout: default
---

# Introduction


These repositories contains deep learning modules to perform a POC of a software in progress that will be able to perform all
image related analysis like VQA, activity classification and depth-map analysis specifically. Also, the queries will be handled by
a chatbot which can be seperate module on its own. Each and every module also is having a command line interface with independant
functioning.   
Ideal functionality for this software is to extract maximum information out of feeded images and its ability to understanding the
context of queries in more natural language manner. Hence, with that aim in mind the combined software is being developed.  

Now carrying on the discussion for specific modules and their functionalities.
These repositories will contain deep learning projects, with their trained models and all the related files to it for use.
Toy projects of deep learning is not aim for this repository collection instead projects with some real implications will
be selected to work with. Other than general information extraction, models are being trained to function around for specific
use-cases in different domains.
Also, alternative approaches to deep learning models their efficiency, ease of use and throughput for particular use-case will
be discussed. Consider, for example a flow based chatbot functioning with mutliple if-else cases but still being able to get the job done. But limited by its inability to handle all the cases in hand. Currently, aim is to restrict ourself to limited four modules only and implement them in different manners,
study their efficiency with deep learning models. Hopefully, create newer model architecture also as per need.

Current use cases under test involves, In Progress:

|   __Modules__               |               __Use Cases__                   |
|:----------------------------|:----------------------------------------------|
| __Chatbot__                 | TroubleShooting, Personality, Recruiting      |
| __VQA__                     | Product, Brand Identification, Recommendation |
| __Activity Classification__ | Audio and Visual Aid, Education               |
| __Depth Map Analysis__      | Realtime Audio and Visual Aid                 |

Also, alterative approaches will be explored for implementing these use-cases. And comparative case-study will be documented.
This website will contain the following content related to project

# Contents 
  * Complete Working Explained
    * Important Code snippets
    * Test Cases
  * Design Diagrams
  
  * UI Design
    * Code
  * Literatue Review
  
  * References, Acknowledgements And Credits
  
* * *

# Working & Functionality


* * *

# Design Diagrams

__Current Design Workings:__ In the working of Deep Vision tool the main UI will prompt options to use a specific module for that particular task. After that selection that particular module will get triggered and control will get transfered to that module for information extraction. Main UI will still work and can launch other submodules.  

![Actvity Diagram](assets/ActvityDiagram.png)

The flow of main UI is highlighted by this diagram. Along, with encoder-decoder process on a trained dataset with annotations already labelled on them.

![Component Diagram](assets/ComponentDiagram.png)

This diagram how this software can be divided into different components in terms of functionality and modular nature to achieve
independent performance. With initial flow based chatbot for querying from the information extraction modules.   

* * *

# UI Design & Functionality

GUI programming is different from control flow programming which is a control flow one but gui programming is __event driven__ programming. Set everything up and wait for event generated by user's actions, then code responds.  

Consider this simple __flow-based__ chatbot with __hard-coded strings__ for input & response with enter button.  

```
def func():
  # flow-logic
  # with if-else & case statements

# initialization of UI & packing user input into UI
root = tk.Tk()
user_input = tk.Entry(root)
user_input.pack()

# packing Button onto UI
button = tk.Button(root, text="Enter", command=func)
button.pack()

# packing output as Label text onto UI
output = tk.Label(root, text='')
output.pack()

# to keep the UI afloat
tk.mainloop()
```

![Flow Based Chatbot](assets/FlowEnterButtonChatbotUI.png)  

But,`Enter` button and hard-coded strings are not ideal. Hence, we can replace hard-coded string with pattern matching with
__regex expressions__ with which we can trigger our individual modules. Also, we can bind callback to the Entry widget
so that it's called when the `Enter key` is pressed inside the Entry widget. Signature of callback modified, as now it will
receive Event Object when it is called.

```
def func():
  # flow-logic with regex expression to trigger modules
...

user_input.bind("<Return>", cb)
output = tk.Label(root, text='')
output.pack()

tk.mainloop()
```

Finally, the UI(_In Progress_) will contain regex pattern matching logic which will trigger on different independent modules in console. Tests for pattern matching logic and final UI demo will be added soon in this site.

* * *

# Testing Plan

* * *

# Literature Survey

1. [10] VQA: Visual Question Answering the task of free-form and open-ended Visual Question
Answering (VQA) is by giving an image and a natural language question about the image, the task is
to provide an accurate natural language answer ​ Pros: In this paper proposed combining an LSTM for
the question with a CNN for the image to generate an answer a similar model is evaluated in this
paper. ​ Cons:​ Accuracy 54.06%

2. [1] Dynamic Memory Networks for Visual and Textual Question Answering We have
proposed new modules for the DMN framework to achieve strong results without supervision of
supporting facts. These improvements include the input fusion layer to allow interactions between
input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs.
Pros:​ architecture, the dynamic memory network(DMN). ​ Cons: ​ Accuracy 60.4%

3. [9] Generative Adversarial Text to Image Synthesis In this work we are interested in
translating text in the form of single-sentence human-written descriptions directly into image pixels.
Pros: develop a novel deep architecture and GAN formulation to effectively bridge these advances in
text and image modelling, translating visual concepts from characters to pixels. ​ Cons: On Close
inspection it is clear that the generated scenes are not usually coherent;

4. [11] Estimated Depth Map Helps Image Classification Therefore, we present a way of
transferring domain knowledge on depth estimation to a separate image classification task over a
disjoint set of train, and test data. ​ Pros: 2-layer feed-forward neural network yielded a performance
increase of 4%, when comparing a NN trained on the RGB dataset to the NN trained on the RGBD
dataset. ​ Cons:​ We get 56% and 52% validation accuracy with RGBD and RGB dataset respectively.

* * *

# References

[1] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and
textual question answering. arXiv preprint arXiv:1603.01417, 2016.  
[2] Donahue, J., Hendricks, L.A., Guadarrama, S., et al.: ‘Long-term recurrent convolutional
networks for visual recognition and description’. IEEE Conf. Computer Vision and Pattern
Recognition (CVPR), 2015  
[3] GloVe: Global Vectors for Word Representation. ​ https://nlp.stanford.edu/projects/glove/​ .  
[4] ​ https://github.com/machrisaa/tensorflow-vgg?files=1  
[5] Karpathy, A., and Fei-Fei, L.: ‘Deep visual-semantic alignments for generating image
descriptions’. The IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 2015  
[6] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.. Szegedy, W. Liu, Y.  
[7] Mansimov, E., Parisotto, E., Ba, J. L., and Salakhutdinov, R. Generating images from captions
with attention. ICLR, 2016.  
[8]M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L.
Kaiser, M. Kudlur, J. Levenberg, D. Man ́e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J.
Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vi ́egas, O.
Vinyals, P. Warden, M.Wattenberg, M.Wicke, Y. Yu, and X. Zheng. Tensor-Flow: Large-scale
machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.  
[9] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran Bernt Schiele, Honglak Lee.
Generative Adversarial Text to Image Synthesis arXiv: 1605.05396v2 [cs.NE] 5 Jun 2016  
[10] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence
Zitnick, Devi Parikh, Virginia Tech Microsoft Research, VQA: Visual Question Answering IEEE
Explore, Year 2015  
[11] Yihui He, Xi’an Jiaotong University, Xi’an, China Estimated Depth Map Helps Image
Classification, arXiv:1709.07077v1 [cs.CV] 20 Sep 2017  

* * *
